{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from newspaper import Article\n",
    "from newspaper import ArticleException\n",
    "from telegram import ReplyKeyboardMarkup, ReplyKeyboardRemove, Update\n",
    "import telegram\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "from tldextract import extract\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "from telegram.ext import (\n",
    "    Updater,\n",
    "    CommandHandler,\n",
    "    MessageHandler,\n",
    "    Filters,\n",
    "    ConversationHandler,\n",
    "    CallbackContext,\n",
    ")\n",
    "from GoogleNews import GoogleNews\n",
    "TOKEN = \"TOKEN CENSORED FOR PRIVATE REASONS\"\n",
    "FILE_LOCATION = \"vc3.sav\"\n",
    "RERUN, WAIT, PREDICT, SUGGEST = range(4)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocess text for higher accuracy\n",
    "def preprocess(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDetector:\n",
    "    def __init__(self, file_location):\n",
    "        # Loads the pre-saved model and googlenews object\n",
    "        self.model = pickle.load(open(file_location, 'rb'))\n",
    "        self.googlenews = GoogleNews(lang='en', encode='utf-8')\n",
    "        self.tfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,4))\n",
    "    \n",
    "    def get_article(self, url):\n",
    "        # Calls the Article library to download and parse the url\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article\n",
    "    \n",
    "    def get_google_results(self, sentence):\n",
    "        # Calls the GoogleNews library to search for nearest results\n",
    "        # Retrieves the nearest url links and stores it in a variable named results\n",
    "        # Clears the googlenews object results\n",
    "        self.googlenews.search(sentence)\n",
    "        results = self.googlenews.get_links()\n",
    "        self.googlenews.clear()\n",
    "        return results\n",
    "        \n",
    "    def suggest_by_keywords(self, keywords, threshold=3):\n",
    "        # Get google results using keywords\n",
    "        # Adds them to a suggestion list if they are reliable\n",
    "        # Returns the list\n",
    "        results = self.get_google_results(keywords)\n",
    "        logger.info(f\"SUGGESTED KEYWORDS : {keywords}\")\n",
    "        suggestions = []\n",
    "        for news_url in results:\n",
    "            logger.info(f\"SUGGESTED NEWS URL : {news_url}\")\n",
    "            result = self.predict(news_url)\n",
    "            if not result :\n",
    "                suggestions.append(news_url)\n",
    "            if len(suggestions) == threshold:\n",
    "                break\n",
    "                \n",
    "        return suggestions\n",
    "    \n",
    "    def suggest_by_article(self, url, threshold=5):\n",
    "        # Retrieves the keywords from article url\n",
    "        # Calls the suggest_by_keyword function and returns the result\n",
    "        article = self.get_article(url)\n",
    "        logger.info(f\"SUGGESTED URL : {url}\")\n",
    "        # Gets the keywords of the paragraph using NLTK and joins them in a sentence\n",
    "        article.nlp()\n",
    "        keywords = article.keywords\n",
    "        sentence = ' '.join(keywords[0:6])\n",
    "        return self.suggest_by_keywords(sentence)\n",
    "    \n",
    "    def predict(self, url):\n",
    "        # Calls a prediction based on the article URL\n",
    "        # Result is unreliable if 1, reliable if 0\n",
    "        try:\n",
    "            article = self.get_article(url)\n",
    "            content = article.text\n",
    "            logger.info(url)\n",
    "            _, td, tsu = extract(url) \n",
    "            domain = td + '.' + tsu\n",
    "            title = article.title\n",
    "            content = preprocess(content)\n",
    "            logger.info(content)\n",
    "            result = self.model.predict([content])\n",
    "            logger.info(result)\n",
    "            if result != \"fake\":\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        except ArticleException:\n",
    "            return -1\n",
    "  \n",
    "ml_model = FakeNewsDetector(FILE_LOCATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(update, context):\n",
    "    # Starting state when calling /start\n",
    "    logger.info(f\"State : INITIALISE\")\n",
    "    reply_keyboard = [['Verify News', 'Suggest News']]\n",
    "    \n",
    "    update.message.reply_text(\n",
    "        \"Hi! I'm a FakeNews Bot!\\n\"\n",
    "        'Send /cancel to stop talking to me.\\n'\n",
    "        \"Here are the things I can do currently :\\n\"\n",
    "        \"1. Predict if a news URL is true/fake\\n\"\n",
    "        \"2. Suggest an article based on keywords\\n\"\n",
    "        'What do you want me to do?',\n",
    "        reply_markup=ReplyKeyboardMarkup(reply_keyboard, one_time_keyboard=True),\n",
    "    )\n",
    "    return WAIT\n",
    "\n",
    "def wait(update, context):\n",
    "    # Waiting state to get the reply from the user\n",
    "    # State then transition either into PREDICT/SUGGEST according to user feedback\n",
    "    logger.info(f\"State : WAIT\")\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"Result of choice {user.first_name} : {update.message.text}\")\n",
    "    if update.message.text == 'Verify News':\n",
    "        update.message.reply_text('What is the article URL?',reply_markup=ReplyKeyboardRemove())\n",
    "        return PREDICT\n",
    "    else:\n",
    "        update.message.reply_text(\"What keywords are you looking for?\\nKeep your keywords space-separated for higher accuracy.\",reply_markup=ReplyKeyboardRemove())\n",
    "        return SUGGEST\n",
    "    \n",
    "def help(update, context):\n",
    "    update.message.reply_text(\"Run /start to get started!\")\n",
    "    \n",
    "def predict(update, context):\n",
    "    # Gets the article URL\n",
    "    # Pretends that the bot is speaking\n",
    "    # Make a prediction\n",
    "    logger.info(f\"State : PREDICT\")\n",
    "    article_url = str(update.message.text)\n",
    "    updater.bot.send_chat_action(chat_id=update.message.chat_id, action=telegram.ChatAction.TYPING)\n",
    "    tag = ml_model.predict(article_url)\n",
    "    \n",
    "    # If tag is 0, reliable\n",
    "    # Else make a suggestion on alternative news sources\n",
    "    if tag == 0:\n",
    "        update.message.reply_text(\"Article seems legit~\")\n",
    "    else:\n",
    "        update.message.reply_text(\"Article seems unreliable. Let me find more reliable sources for you~\")\n",
    "        # Pretend to type to mask the delay in predictions\n",
    "        updater.bot.send_chat_action(chat_id=update.message.chat_id, action=telegram.ChatAction.TYPING)\n",
    "        suggested_article_urls = ml_model.suggest_by_article(article_url)\n",
    "        if len(suggested_article_urls) == 0:\n",
    "            update.message.reply_text(\"There does not seem to be any reliable sources to recommend :(\")\n",
    "        else :\n",
    "            update.message.reply_text(\"Here are the top alternative sources\")\n",
    "            for url in suggested_article_urls:\n",
    "                updater.bot.send_chat_action(chat_id=update.message.chat_id, action=telegram.ChatAction.TYPING)\n",
    "                update.message.reply_text(url)\n",
    "    \n",
    "    return ConversationHandler.END\n",
    "    \n",
    "def suggest(update, context):\n",
    "    # Make a suggestion based on keywords\n",
    "    logger.info(f\"State : SUGGEST\")\n",
    "    \n",
    "    keywords = str(update.message.text)\n",
    "    updater.bot.send_chat_action(chat_id=update.message.chat_id, action=telegram.ChatAction.TYPING)\n",
    "    results = ml_model.suggest_by_keywords(keywords)\n",
    "    update.message.reply_text(\"Here are the top 3 reliable articles\")\n",
    "    for url in results:\n",
    "        updater.bot.send_chat_action(chat_id=update.message.chat_id, action=telegram.ChatAction.TYPING)\n",
    "        update.message.reply_text(url)\n",
    "    return ConversationHandler.END\n",
    "\n",
    "def echo(update, context):\n",
    "    update.message.reply_text(update.message.text)\n",
    "\n",
    "def error(update, context):\n",
    "    logger.warning(f\"Update {str(update)} caused error {str(context.error)}\")\n",
    "    \n",
    "def cancel(update, context) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(\"User %s canceled the conversation.\", user.first_name)\n",
    "    update.message.reply_text(\n",
    "        'Bye! I hope we can talk again some day.', reply_markup=ReplyKeyboardRemove()\n",
    "    )\n",
    "    return ConversationHandler.END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = Updater(TOKEN, use_context=True)\n",
    "\n",
    "# Get the dispatcher to register handlers\n",
    "dp = updater.dispatcher\n",
    "\n",
    "\n",
    "conv_handler = ConversationHandler(\n",
    "    entry_points=[CommandHandler('start', initialise)],\n",
    "    states = {\n",
    "        WAIT : [MessageHandler(Filters.regex('^(Verify News|Suggest News)$'), wait)],\n",
    "        PREDICT : [MessageHandler(Filters.text, predict)],\n",
    "        SUGGEST : [MessageHandler(Filters.text, suggest)]\n",
    "    },\n",
    "    fallbacks=[CommandHandler('cancel', cancel)]\n",
    ")\n",
    "\n",
    "dp.add_handler(conv_handler)\n",
    "dp.add_handler(CommandHandler('help', help))\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  }
 ]
}